# Introduction
After I've learned the theory behind the computer vision subfield, I moved to the next subfield which is the natural language processing (NLP), when the main aim of the project was to translate all the theory into something practical in order to strengthen the understanding.

Additional topics that I learned were RNN, LSTM, GRU, and Transformers.

# Performance
Unlike the computer vision project, here there is not much data to train the neural network on, so I was able to train the neural network along a lot of epochs without any computational restrictions.

The neural network was able to achieve 100% of accuracy on the training data, which means it will be able to generate the corresponding response if the user's input text will be as similar to the patterns the neural network was trained on.

* Accuracy: 16.67% -> 100.00%
* Loss: 1.8099 -> 0.0867

# Demonstration
https://user-images.githubusercontent.com/68182283/187167635-fe8ecab5-692f-423d-953d-990eab3aa150.mp4
